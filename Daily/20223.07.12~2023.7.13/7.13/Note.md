## 7.13学习记录

### RNN--循环神经网络

定义：RNN是一类处理序列数据的神经网络

重要思想：

+ 参数共享：在模型的不同部分使用相同的参数来泛化模型
+ 相关想法：卷积处理一维时间序列 --时延神经网络的基础，但是RNN与其不同的是其以更深的计算图共享，也就是输出的每一项是前一项的函数

展开计算图:

+ 定义：形式化一组计算结构的方式：如将输入和参数映射到输出和损失的计算 
+ RNN是展开递归或循环计算得到的重复结构 ---这些重复结构构成一个事件链
+ 动态系统：$ s^{(t)} = f(s^{(t-1)};\Theta)$, 然后该系统可以通过递归来展开
+ 我们可以把输入x比作外部信号：$ s^{(t)} = f(s^{(t-1)}, x^{(t)};\Theta)$, 其中的过去时间序列是有限程度
+ 展开计算的优点：
  + 无论序列的长度都可以通过递归来实现，是从一种状态到另外一种状态，而不是在可变长度的历史状态上
  + 可以使用相同的函数f

循环神经网络：

+ 重要的设计模式：
  + 每个时间步都有输出，并且隐藏单元之间有循环连接的循环网络
    + 数学原理
    + 损失函数 -- L为给定的X后y的负对数似然
    + 该设计模式需要很高的计算成本，是通过时间方向传播，无法并行化计算
  + 每个时间步都产生一个输出，只有当前时刻的输出到下一个时刻的隐藏单元之间有链接的神经网络
  + 隐藏单元之间存在循环连接，但读取整个序列后产生单个输出的循环网络
+ 两种特殊形式
  + 导师驱动过程
    + 为上述第二种设计模式，解耦时间，可以并行化计算
    + 使用条件最大似然准则
    + 此方法可以与第一种，也就是BPTT算法结合
  + 输出神经网络
    + 一般用上述导师驱动和自由输入来共同训练模型
+ 循环神经网络的梯度
  + 推导过程
+ 作为有向图的循环网络 ----循环网络中的图原理

  + 一般将真实值作为条件，因为其蕴含着过去的信息，而由模型训练处的输出与下一时间的隐藏条件独立
  + 证明过程：
  + 可通过有向图来描述上述问题：
    + 第一种：将任意一对y值作有向边进行连接，忽略隐藏单元h
    + 第二种：将h作为随机变量 ---产生有效的参数化
      + 将h作为过去的y对当前y的中间变量，以此进行联系
    + 三个选择来完整描述图模型：
      + 设置特殊符号训练，产生该符号则停止采样
      + 引入额外的Bernoulli输出，通过训练输出来判断是否停止
      + 将额外的输出添加模型并且预测整数t来采样
+ 基于上下文的RNN序列建模

  + 使用x为一个固定大小的输入来作为额外输入
    + 在每个时刻作为一个额外输入
    + 作为初始状态h(0)
    + 结合两种方式
  + 使用向量序列来作为额外输入
    + 此种方法要在输出和下一个时间序列上进行连接来取消条件独立性
+ 双向RNN

  + 定义：从未来获取信息来预测
+ 基于编码-解码的序列到序列架构

  + 编码器训练最大似然的状态来得到最后的状态h并且作为C输入到解码器中
  + 其中的状态是更新后，变化后的长度向量
  + 可用attention注意力机制
+ 深度循环网络

  + 加入了更深层的隐藏单元，隐藏时间序列
  + 可以引入跳跃连接来缓解路径延长的效应
+ 递归神经网络

  + 一般结构是树状的
  + 减小时间阀度
+ 挑战

  + 梯度爆炸和消失
+ 回声状态网络 --更好的训练参数

  + 简单概念：Jacobian矩阵与谱半径，偏差，线性与非线性映射
  + 策略：固定权重，使其有一定的谱半径，当前向传播时，会由于饱和非线性单元的稳定作用不会爆炸
+ 渗漏单元和其他多时间尺度的策略

  + 时间维度的跳跃连接  ---加入延迟
  + 渗漏单元   --滑动平均值
    + 其中渗漏单元可以模拟滑动平均值的效益，以此来进行对信息的记忆和遗忘
    + 更新模式：$\mu^{(t)} \to \alpha\mu^{(t-1)} + (1-\alpha)\vartheta^{(t)}$
  + 删除连接
    + 不同于跳跃连接，此为更改连接长度改为更长的连接
      + 一种是把循环单元改为渗透连接
      + 第二种是使显式且离散的更新发生在不同的时间


### 长短期记忆LSTM和其他门控RNN

- LSTM
  - 更新方式：$s_i^{(t)} = f_i^{(t)}s_i{(t-1)} + g_i^{(t)}\sigma(b_i+\sum_j U_{i,j}x_j^{(t)}+\sum_jW_{i,j}h_j^{(t-1)})$

+ 其他门控RNN
  + 更新门
  + 复位门
+ 优化长期依赖

### Transform模型

基础知识学习：

1. 残差网络（残差连接）：一个残差块的表示：$x_{l+1} = x_l + F(x_1,W_l)$ (PS:这是维度一致的情况下，否者可以用$h(x_l)$来表示) ， 然后多个残差块就可以构成一个残差网络，为了防治梯度消失
2. Mask：掩盖时间序列后端的数据，赋值为很大的负数
3. embedding：扩展维度，或者为维度表示
4. Positional Encodiing：输入加入时序信息 
5. drop：正则化
6. label smoothing ： 类似置信度

基本框架：
![](https://gitee.com/eslinyi/picture/raw/master/img/20230713144150.png)

其中的multi-head Attention注意力机制多加了不同通道来进行

### 代码学习

1. BP神经网络

   + 重新细致看了上次的代码，发现了一些问题，就是代码实现的函数和理论不一样，缺少了一个参数，具体可看代码，但是按照原理来观察了代码之后发现其准确率反而下降了，所以......不知道是不是我理解有误还是说缺少了那个值会更加好

2. CNN卷积神经网络

   卷积层原理：$\text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) + \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)$

   为了更加深入的理解，我去寻找卷积网络的源代码，观察是如何实现卷积网络的，但是好像找不到，不过其实我自己可以敲一个出来，具体可以参考代码 ---虽然可能还没有完全研究出来

### 基本数学模型

今天看了会数学建模的内容，看了前面十几集，了解了一下论文的制作和其他图表规范等内容